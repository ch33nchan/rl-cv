<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>TinyGrad RLCV Mathematical Foundation</title>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 20px;
            color: #333;
        }
        .container {
            max-width: 900px;
            margin: 0 auto;
        }
        h1, h2, h3 {
            color: #2c3e50;
        }
        .math-block {
            background-color: #f8f9fa;
            padding: 15px;
            border-radius: 5px;
            margin: 20px 0;
        }
        .algorithm {
            background-color: #f0f4f8;
            padding: 15px;
            border-left: 4px solid #3498db;
            margin: 20px 0;
        }
        .note {
            background-color: #fff8e1;
            padding: 10px;
            border-left: 4px solid #ffc107;
            margin: 15px 0;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>TinyGrad RLCV: Mathematical Foundation</h1>
        
        <p>This document explains the mathematical principles behind the TinyGrad Reinforcement Learning Computer Vision (RLCV) system.</p>
        
        <h2>1. Reinforcement Learning Fundamentals</h2>
        
        <h3>1.1 Markov Decision Process (MDP)</h3>
        <p>The reinforcement learning problem is formalized as a Markov Decision Process (MDP), defined by the tuple \((S, A, P, R, \gamma)\):</p>
        <div class="math-block">
            <p>
                \(S\): State space<br>
                \(A\): Action space<br>
                \(P(s'|s,a)\): Transition probability function<br>
                \(R(s,a,s')\): Reward function<br>
                \(\gamma \in [0,1]\): Discount factor
            </p>
        </div>
        
        <h3>1.2 Value Functions</h3>
        <p>The state-value function \(V^\pi(s)\) represents the expected return starting from state \(s\) and following policy \(\pi\):</p>
        <div class="math-block">
            \[V^\pi(s) = \mathbb{E}_\pi \left[ \sum_{t=0}^{\infty} \gamma^t R_{t+1} | S_0 = s \right]\]
        </div>
        
        <p>The action-value function \(Q^\pi(s,a)\) represents the expected return starting from state \(s\), taking action \(a\), and then following policy \(\pi\):</p>
        <div class="math-block">
            \[Q^\pi(s,a) = \mathbb{E}_\pi \left[ \sum_{t=0}^{\infty} \gamma^t R_{t+1} | S_0 = s, A_0 = a \right]\]
        </div>
        
        <h3>1.3 Bellman Equations</h3>
        <p>The Bellman expectation equation for \(Q^\pi\):</p>
        <div class="math-block">
            \[Q^\pi(s,a) = \mathbb{E}_{s'} \left[ R(s,a,s') + \gamma \sum_{a'} \pi(a'|s') Q^\pi(s',a') \right]\]
        </div>
        
        <p>The Bellman optimality equation for \(Q^*\):</p>
        <div class="math-block">
            \[Q^*(s,a) = \mathbb{E}_{s'} \left[ R(s,a,s') + \gamma \max_{a'} Q^*(s',a') \right]\]
        </div>
        
        <h2>2. Deep Q-Network (DQN)</h2>
        
        <h3>2.1 Q-Learning with Function Approximation</h3>
        <p>In DQN, we approximate the Q-function using a neural network with parameters \(\theta\):</p>
        <div class="math-block">
            \[Q(s,a;\theta) \approx Q^*(s,a)\]
        </div>
        
        <h3>2.2 Loss Function</h3>
        <p>The DQN loss function is based on the temporal difference error:</p>
        <div class="math-block">
            \[L(\theta) = \mathbb{E}_{(s,a,r,s')} \left[ \left( r + \gamma \max_{a'} Q(s',a';\theta^-) - Q(s,a;\theta) \right)^2 \right]\]
        </div>
        <p>where \(\theta^-\) are the parameters of a target network that is updated less frequently to improve stability.</p>
        
        <h3>2.3 Experience Replay</h3>
        <p>Experience replay stores transitions \((s_t, a_t, r_t, s_{t+1})\) in a replay buffer and samples them randomly for training, which helps break correlations between consecutive samples.</p>
        
        <div class="algorithm">
            <h4>DQN Algorithm</h4>
            <ol>
                <li>Initialize replay memory \(D\) to capacity \(N\)</li>
                <li>Initialize action-value function \(Q\) with random weights \(\theta\)</li>
                <li>Initialize target action-value function \(\hat{Q}\) with weights \(\theta^- = \theta\)</li>
                <li>For episode = 1 to M:
                    <ol>
                        <li>Initialize state \(s_1\)</li>
                        <li>For t = 1 to T:
                            <ol>
                                <li>With probability \(\epsilon\) select random action \(a_t\), otherwise \(a_t = \arg\max_a Q(s_t, a; \theta)\)</li>
                                <li>Execute action \(a_t\) and observe reward \(r_t\) and next state \(s_{t+1}\)</li>
                                <li>Store transition \((s_t, a_t, r_t, s_{t+1})\) in \(D\)</li>
                                <li>Sample random minibatch of transitions \((s_j, a_j, r_j, s_{j+1})\) from \(D\)</li>
                                <li>Set \(y_j = r_j + \gamma \max_{a'} \hat{Q}(s_{j+1}, a'; \theta^-)\)</li>
                                <li>Perform gradient descent step on \((y_j - Q(s_j, a_j; \theta))^2\) with respect to \(\theta\)</li>
                                <li>Every \(C\) steps reset \(\hat{Q} = Q\)</li>
                            </ol>
                        </li>
                    </ol>
                </li>
            </ol>
        </div>
        
        <h2>3. Computer Vision Components</h2>
        
        <h3>3.1 Convolutional Neural Networks</h3>
        <p>The convolutional layer operation is defined as:</p>
        <div class="math-block">
            \[z_{i,j,k} = \sum_{l=1}^{C_{in}} \sum_{m=0}^{f-1} \sum_{n=0}^{f-1} x_{i+m,j+n,l} \cdot w_{m,n,l,k} + b_k\]
        </div>
        <p>where \(x\) is the input, \(w\) is the filter weights, \(b\) is the bias, \(f\) is the filter size, \(C_{in}\) is the number of input channels, and \(z\) is the output feature map.</p>
        
        <h3>3.2 Feature Extraction</h3>
        <p>In our lightweight model, we use a series of convolutional layers with ReLU activations:</p>
        <div class="math-block">
            \[h_1 = \text{ReLU}(\text{Conv}(x))\]
            \[h_2 = \text{ReLU}(\text{Conv}(h_1))\]
            \[h_3 = \text{ReLU}(\text{Conv}(h_2))\]
        </div>
        
        <h3>3.3 Object Detection Metrics</h3>
        <p>Intersection over Union (IoU) measures the overlap between predicted and ground truth bounding boxes:</p>
        <div class="math-block">
            \[\text{IoU} = \frac{\text{Area of Overlap}}{\text{Area of Union}}\]
        </div>
        
        <h2>4. TinyGrad Optimization</h2>
        
        <h3>4.1 Automatic Differentiation</h3>
        <p>TinyGrad uses reverse-mode automatic differentiation to compute gradients efficiently:</p>
        <div class="math-block">
            \[\frac{\partial L}{\partial x_i} = \sum_j \frac{\partial L}{\partial y_j} \frac{\partial y_j}{\partial x_i}\]
        </div>
        
        <h3>4.2 Adam Optimizer</h3>
        <p>The Adam optimizer updates parameters using first and second moments of gradients:</p>
        <div class="math-block">
            \[m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t\]
            \[v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2\]
            \[\hat{m}_t = \frac{m_t}{1 - \beta_1^t}\]
            \[\hat{v}_t = \frac{v_t}{1 - \beta_2^t}\]
            \[\theta_t = \theta_{t-1} - \alpha \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}\]
        </div>
        <p>where \(g_t\) is the gradient, \(m_t\) and \(v_t\) are the first and second moment estimates, \(\alpha\) is the learning rate, and \(\beta_1, \beta_2, \epsilon\) are hyperparameters.</p>
        
        <h2>5. Lightweight Model Design</h2>
        
        <h3>5.1 Model Efficiency Metrics</h3>
        <p>We measure model efficiency using the following metrics:</p>
        <div class="math-block">
            <p>
                FLOPs: Number of floating-point operations<br>
                Parameters: Number of trainable parameters<br>
                Memory footprint: Size of the model in memory<br>
                Inference time: Time to process one input
            </p>
        </div>
        
        <h3>5.2 Quantization</h3>
        <p>Quantization reduces precision of weights from 32-bit floating point to lower bit representations:</p>
        <div class="math-block">
            \[w_q = \text{round}\left(\frac{w - w_{min}}{w_{max} - w_{min}} \cdot (2^b - 1)\right) \cdot \frac{w_{max} - w_{min}}{2^b - 1} + w_{min}\]
        </div>
        <p>where \(w_q\) is the quantized weight, \(w\) is the original weight, \(w_{min}\) and \(w_{max}\) are the minimum and maximum values, and \(b\) is the bit-width.</p>
        
        <div class="note">
            <p><strong>Note:</strong> The mathematical formulations presented here are implemented in the TinyGrad RLCV system with optimizations for mobile and ARM-based devices. The actual implementation may use approximations or alternative formulations to improve efficiency.</p>
        </div>
    </div>
</body>
</html>